\section{Related Work}
Classical neural network training relies on gradient-based optimization~\citep{rumelhart1986learning}. Convolutional architectures dominated vision benchmarks beginning with LeNet~\citep{lecun1998gradient} and surged after ImageNet results by AlexNet~\citep{krizhevsky2012imagenet}. In NLP, recurrent architectures gave way to self-attention models, culminating in the Transformer~\citep{vaswani2017attention}, BERT~\citep{devlin2018bert}, and large autoregressive models~\citep{brown2020language}.

Surveys and textbooks provide broader context, but here we keep the scope concise and refer readers to the cited works for details.
